{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/40067/confusion-matrix-three-classes-python/40068 \n",
    "#confusion matrix graphics code\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True Growth')\n",
    "    plt.xlabel('Predicted Growth')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest code\n",
    "import pandas as pd\n",
    "\n",
    "#format input\n",
    "input1= 'Y1000KEGG_m.csv'\n",
    "input2='final_KEGG_carb.csv'\n",
    "\n",
    "df1 = pd.read_csv(input1, index_col=0)\n",
    "df2 = pd.read_csv(input2, index_col=0)\n",
    "df = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "\n",
    "\n",
    "#import modules needed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import compute_sample_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "#define what we want to predict out of the dataset\n",
    "predicting = 'Carb_FINAL'\n",
    "#drop NAs (species we didn't measure specialism/generalism) from the dataset\n",
    "df = df.dropna(subset=[predicting])\n",
    "#drop species that we did measure specialism/generalism but fall in the \"normal\" category and are not specialists or generalists \n",
    "df = df[df[predicting] != 0]\n",
    "df[predicting] = df[predicting].replace([2],0)\n",
    "\n",
    "#decide what other data you want to drop from the input, here I am dropping the predicted value\n",
    "ml_input = np.array(input_ed) #array of input\n",
    "output = np.array(df[predicting]) #array of output\n",
    "print(output)\n",
    "\n",
    "#build your model \n",
    "model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss', n_jobs = 8)\n",
    "\n",
    "#use cross_val_predict to test the model and build the big confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred = cross_val_predict(model, ml_input, output, cv=10)\n",
    "conf_mat = confusion_matrix(output, y_pred)\n",
    "print(conf_mat)\n",
    "plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_mat, classes=['0','1'],\n",
    "                     title='Confusion matrix')\n",
    "plt.savefig(\"Confusion_Matrix.png\", dpi=200)\n",
    "\n",
    "\n",
    "#look specifically at which species are incorrectly and correctly predicted\n",
    "species_list = list(df.index)\n",
    "print('Incorrectly IDed  True_Value  Predicted_Value')\n",
    "for i in range(0, len(species_list)):\n",
    "    if output[i] != y_pred[i]:\n",
    "        print(species_list[i], output[i], y_pred[i])\n",
    "    else:\n",
    "        pass\n",
    "print('Correctly IDed   True_Value  Predicted_Value')\n",
    "for i in range(0, len(species_list)):\n",
    "    if output[i] == 1 and y_pred[i] == 1:\n",
    "        print(species_list[i], output[i], y_pred[i])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "header = list(input_ed.columns)\n",
    "\n",
    "# define the model again to re-test on a held out dataset, usually have  subsample=0.9, colsample_bynode=0.8, \n",
    "model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss', n_jobs = 8)\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(ml_input, output, test_size=0.10)\n",
    "class_weights_training = compute_sample_weight('balanced', ytrain)\n",
    "#sample_weight=class_weights_training\n",
    "#model.fit(xtrain, ytrain) \n",
    "model.fit(xtrain, ytrain, sample_weight=class_weights_training) \n",
    "\n",
    "print('Mini_test')\n",
    "ypred = model.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "total = sum(cm[0]) + sum(cm[1])\n",
    "mis = cm[0][1] + cm[1][0]\n",
    "print('Misclassification rate:', mis/total)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=['0','1'],\n",
    "                     title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "#rebuild model yet again for the final big cross validation accuracy metrics with RepeatedStratifiedKFold\n",
    "classes_weights = compute_sample_weight('balanced', output)\n",
    "model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss', n_jobs = 8)\n",
    "sample_weight=classes_weights\n",
    "model.fit(ml_input, output, sample_weight=classes_weights)\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, ml_input, output, scoring='balanced_accuracy', cv=cv)\n",
    "print(n_scores)\n",
    "# report performance\n",
    "print('Balanced Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)), '\\n')\n",
    "\n",
    "\n",
    "#report feature importances in order of most important to least\n",
    "importance_list = []\n",
    "importances = list(model.feature_importances_)\n",
    "o = 0\n",
    "m = 0\n",
    "top10 = []\n",
    "for i in model.feature_importances_:\n",
    "    importance_list.append([i, header[o]])\n",
    "    o = o + 1\n",
    "for i in importance_list:\n",
    "    if i[0] == max(importances):\n",
    "        print(i[1], '\\t', i[0])\n",
    "        if m < 10:\n",
    "            top10.append(i[1])\n",
    "            m = m + 1\n",
    "            importances.remove(i[0])\n",
    "        else:\n",
    "            importances.remove(i[0])\n",
    "    else:\n",
    "        importance_list.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC_AUC code \n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import compute_sample_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('final_KEGG_carb.csv', index_col=0)\n",
    "df2 = pd.read_csv('Y1000KEGG_m.csv', index_col=0)\n",
    "df = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "\n",
    "predicting = 'Carb_FINAL'\n",
    "df = df.dropna(subset=[predicting])\n",
    "df = df[df[predicting] != 0]\n",
    "df[predicting] = df[predicting].replace([2],0)\n",
    "\n",
    "input_ed = df.drop(['Carb_FINAL'], axis=1)\n",
    "\n",
    "X = np.array(input_ed) #array of input\n",
    "y = np.array(df[predicting])\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss', n_jobs=8)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "    model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss',  n_jobs=8)\n",
    "    model.fit(X[train], y[train])\n",
    "    y_score = model.predict_proba(X[test])\n",
    "    fpr, tpr, _ = roc_curve(y[test], y_score[:, 1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, 'b', alpha=0.15)\n",
    "    tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "aucs = np.array(aucs)\n",
    "mean_auc = aucs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "\n",
    "\n",
    "plt.plot(base_fpr, mean_tprs, 'b', label = 'Mean ROC (AUC = %0.2f)' % mean_auc )\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3, label=r\"$\\pm$ 1 std. dev.\")\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--', label=\"Chance\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig(\"gen_spec.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to recursively run the random forest while dropping the 20 most important traits to see how many traits are truly important\n",
    "def randomforest(file1, file2, predicting, drop):\n",
    "    import pandas as pd\n",
    "    df1 = pd.read_csv(file1, index_col=0)\n",
    "    df2 = pd.read_csv(file2, index_col=0)\n",
    "    df = pd.concat([df1, df2.set_index(df1.index)], axis=1)\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "    from xgboost import XGBRFClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import xgboost as xgb\n",
    "    from sklearn.utils import compute_sample_weight\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = df.dropna(subset=[predicting])\n",
    "    df = df[df[predicting] != 0]\n",
    "    df[predicting] = df[predicting].replace([2],0)\n",
    "    input_ed = df.drop(drop, axis=1)\n",
    "    ml_input = np.array(input_ed) #array of input\n",
    "    output = np.array(df[predicting]) #array of output\n",
    "    model = XGBRFClassifier(max_depth=12, n_estimators=100, use_label_encoder =False,  eval_metric='mlogloss', n_jobs = 8)\n",
    "    header = list(input_ed.columns)\n",
    "    xtrain, xtest, ytrain, ytest=train_test_split(ml_input, output, test_size=0.10)\n",
    "    class_weights_training = compute_sample_weight('balanced', ytrain)\n",
    "    model.fit(xtrain, ytrain, sample_weight=class_weights_training) \n",
    "    #ypred = model.predict(xtest)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(model, ml_input, output, scoring='balanced_accuracy', cv=cv)\n",
    "    print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)), '\\n')\n",
    "    importance_list = []\n",
    "    importances = list(model.feature_importances_)\n",
    "    o = 0\n",
    "    m = 0\n",
    "    top2 = []\n",
    "    for i in model.feature_importances_:\n",
    "        importance_list.append([i, header[o]])\n",
    "        o = o + 1\n",
    "    for i in importance_list:\n",
    "        if i[0] == max(importances):\n",
    "            if m < 20:\n",
    "                print(i[1], '\\t', i[0])\n",
    "                top2.append(i[1])\n",
    "                m = m + 1\n",
    "                importances.remove(i[0])\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            importance_list.append(i)\n",
    "    for i in top2:\n",
    "        drop.append(i)\n",
    "    randomforest(file1,file2,file3,predicting,drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the recursive function\n",
    "randomforest('Y1000KEGG_m.csv','final_KEGG_carb.csv','Carb_FINAL', ['Carb_FINAL'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-data-science-xgb",
   "language": "python",
   "name": "py3-data-science-xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
